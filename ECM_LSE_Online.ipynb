{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECM_LSE_Online.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM80t1wl7Y/FaaYCxFD6WG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/ECM-LSE/blob/master/ECM_LSE_Online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEJa2_TZnec"
      },
      "source": [
        "## **ECM-LSE: Prediction of Extracellular Matrix Proteins Using Deep Latent Space Encoding of Composition of k-Spaced Amino Acid Pairs**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-m_byicZkQc"
      },
      "source": [
        "This code provide python implementation of ECM-LSE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadaflfshqgF"
      },
      "source": [
        "# Loading Useful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6012nzZktl"
      },
      "source": [
        "## Load useful packages\n",
        "import sys, os, re, gc\n",
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import optimizers\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer, Input, Dense, BatchNormalization, Dropout\n",
        "from keras import optimizers\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from random import sample\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "from keras.constraints import min_max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwrowDzZ_Ah"
      },
      "source": [
        "# Feature-Extraction\n",
        "\n",
        "The CKSAAP feature encoding calculates the frequency of amino acid pairs separated by any k residues. The CKSAAP encoding scheme reflects the amino acid pair information in small and large range with in the peptides depending upon the value of k(gap). The encoding scheme is utilized from iFeature web server, using the following download link: (https://github.com/Superzchen/iFeature).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGqCLBjmgNDI"
      },
      "source": [
        "## Define CKSAAP feature-extraction function\n",
        "def minSequenceLength(fastas):\n",
        "\tminLen = 10000\n",
        "\tfor i in fastas:\n",
        "\t\tif minLen > len(i[1]):\n",
        "\t\t\tminLen = len(i[1])\n",
        "\treturn minLen\n",
        "\n",
        "def CKSAAP(fastas, gap=5, **kw):\n",
        "\tif gap < 0:\n",
        "\t\tprint('Error: the gap should be equal or greater than zero' + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tif minSequenceLength(fastas) < gap+2:\n",
        "\t\tprint('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tAA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\tencodings = []\n",
        "\taaPairs = []\n",
        "\tfor aa1 in AA:\n",
        "\t\tfor aa2 in AA:\n",
        "\t\t\taaPairs.append(aa1 + aa2)\n",
        "\theader = ['#']\n",
        "\tfor g in range(gap+1):\n",
        "\t\tfor aa in aaPairs:\n",
        "\t\t\theader.append(aa + '.gap' + str(g))\n",
        "\tencodings.append(header)\n",
        "\tfor i in fastas:\n",
        "\t\tname, sequence = i[0], i[1]\n",
        "\t\tcode = [name]\n",
        "\t\tfor g in range(gap+1):\n",
        "\t\t\tmyDict = {}\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tmyDict[pair] = 0\n",
        "\t\t\tsum = 0\n",
        "\t\t\tfor index1 in range(len(sequence)):\n",
        "\t\t\t\tindex2 = index1 + g + 1\n",
        "\t\t\t\tif index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\n",
        "\t\t\t\t\tmyDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
        "\t\t\t\t\tsum = sum + 1\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tcode.append(myDict[pair] / sum)\n",
        "\t\tencodings.append(code)\n",
        "\treturn encodings\n",
        "\n",
        "def getSeq(dataset):\n",
        "\tseq=[]\n",
        "\tfor index, row in dataset.iterrows():\n",
        "\t\tarray = [row['Class'], row['Sequence']]\n",
        "\t\tname, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n",
        "\t\tseq.append([name, sequence])\n",
        "\treturn seq\n",
        "\n",
        "def getCKSAAP(seq,Gap):\n",
        "\tcksaapfea = []\n",
        "\tfor i in seq:\n",
        "\t\ttemp= CKSAAP([i], gap=Gap)\n",
        "\t\tcksaapfea.append(temp)\n",
        "\n",
        "\tdt = []\n",
        "\tfor i in range(len(cksaapfea)):\n",
        "\t\ttemp = cksaapfea[i][1][1:]\n",
        "\t\tdt.append(temp)\n",
        "\tdtn = np.array(dt)\t\n",
        "\treturn dtn\n",
        "\n",
        "# Build dataset using CKSAAP features\n",
        "def ExtractFeatures(dataset,Gap):\n",
        "\tseq=[]\n",
        "\tseq = getSeq(dataset)\n",
        "\tfeatures = getCKSAAP(seq,Gap)\n",
        "\tlabels = dataset['Class']\n",
        "\tlabels[labels=='ECM']=1\n",
        "\tlabels[labels=='NON-ECM']=0\n",
        "\tlabels = to_categorical(labels)\n",
        "\treturn features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9mtHsEaAyA"
      },
      "source": [
        "# Designing an Auto-Encoder-based classifier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge7uF0idR-Gc"
      },
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def ECM_LSE_Final_Model(LV=5,input_size=3600):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(input_size,), name='enc_input')\n",
        "    enc_l1 = Dense(50, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate=0.3)(enc_l1)\n",
        "\n",
        "    enc_l2 = Dense(10, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate=0.3)(enc_l2)\n",
        "\n",
        "    encoder_output = Dense(LV, activation='sigmoid', name='encoder_output')(enc_l2)\n",
        "\n",
        "    # Classifier Network\n",
        "    class_l1 = Dense(10, activation='relu', name='class_layer1')(encoder_output)\n",
        "    class_l2 = Dense(10, activation='relu', name='class_layer2')(class_l1)\n",
        "    class_l3 = Dense(10, activation='relu', name='class_layer3')(class_l2)\n",
        "    class_output = Dense(2, activation='softmax', name='class_output')(class_l3)\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(10, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(50, activation='relu', name='decoder_layer4')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(0.3)(dec_l2)\n",
        "\n",
        "    decoder_output = Dense(input_size, activation='sigmoid', name='decoder_output')(dec_l2)\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[class_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'class_output': 'binary_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  loss_weights={'class_output': 0.01, 'decoder_output': 0.99},\n",
        "                  metrics=[metrics.categorical_accuracy])\n",
        "    # Here I used adam optimizer with default values, two objective functions are optimized\n",
        "    # using  weight factors [1 for classifier and 0.1 for decoder loss]\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83mfGGGKaBdj"
      },
      "source": [
        "## Define performance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsirQcmlTK7y"
      },
      "source": [
        "## Define performance measures\n",
        "def yoden_index(y, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "  j = (tp/(tp+fn)) + (tn/(tn+fp)) - 1\n",
        "  return j\n",
        "\n",
        "def pmeasure(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    f1score = 2 * tp / (2 * tp + fp + fn)\n",
        "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})\n",
        "\n",
        "def Show_Statistics(msg,Stats):\n",
        "  print(msg.upper())\n",
        "  print(70*'-')\n",
        "  print('Accuracy:',Stats[0])\n",
        "  print('Sensitivity:',Stats[1])\n",
        "  print('Specificity:',Stats[2])\n",
        "  print('F1-Score:',Stats[3])\n",
        "  print('MCC:',Stats[4])\n",
        "  print('Balance Accuracy:',Stats[5])\n",
        "  print('Youden-Index:',Stats[6])\n",
        "  print('Reconstruction MSE:',Stats[7])\n",
        "  print(70*'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7frgGLSzaC1p"
      },
      "source": [
        "## Loading and pre-processing ECM prediction dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbQo93M8Obss"
      },
      "source": [
        "## Loading and pre-processing prediction dataset\n",
        "data_path = 'https://raw.githubusercontent.com/Shujaat123/ECM-LSE/master/Dataset.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooB2PHJbaD16"
      },
      "source": [
        "  ## Perform Monte-Carlos Simulations for [num_Trials]\\# of independent Trials\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW4-Y7y-Q28O"
      },
      "source": [
        "## Perform Monte-Carlos Simulations for [num_Trials]# of independent Trials\n",
        "Gaps = [0,1,2,3,4,5,6,7,8,9,10]\n",
        "LVs = [2,3,4,5,6,7,8,9]\n",
        "num_Trails = 20\n",
        "\n",
        "for Gap in Gaps:\n",
        "  dataset_training = pd.read_csv(data_path, index_col=None)\n",
        "  trainDB_features, trainDB_labels = ExtractFeatures(dataset_training,Gap)\n",
        "\n",
        "  plist_training = list(np.asarray(np.where(trainDB_labels[:,1]==1)).flatten())\n",
        "  nlist_training = list(np.asarray(np.where(trainDB_labels[:,1]==0)).flatten())\n",
        "\n",
        "  for LV in LVs:\n",
        "    Stats =[]\n",
        "    for loop_ind in range(0,num_Trails):\n",
        "        ## train\n",
        "        p_train = sample(plist_training, 270)\n",
        "        n_train = sample(nlist_training, 270)\n",
        "        train_list = p_train + n_train\n",
        "        X_train = trainDB_features[train_list]\n",
        "        y_train = trainDB_labels[train_list]\n",
        "\n",
        "        ## valid\n",
        "        p_val = list(set(plist_training) - set(p_train))\n",
        "        p_val = sample(p_val,30)\n",
        "        n_val = list(set(nlist_training) - set(n_train))\n",
        "        n_val = sample(n_val,810)\n",
        "        val_list = p_val + n_val\n",
        "        X_val = trainDB_features[val_list]\n",
        "        y_val = trainDB_labels[val_list]   \n",
        "\n",
        "        ## test\n",
        "        p_test = list(set(plist_training) - set(p_train) - set(p_val))\n",
        "        # p_test = sample(p_test,30)\n",
        "        n_test = list(set(nlist_training) - set(n_train) - set(n_val))\n",
        "        # n_test = sample(n_test,30)\n",
        "        test_list = p_test + n_test\n",
        "        X_test = trainDB_features[test_list]\n",
        "        y_test = trainDB_labels[test_list]\n",
        "        \n",
        "        print('Training Dataset contain:\\n', len(p_train),'ECMs\\t',len(n_train),'Non-ECMs')\n",
        "        print('Validation Dataset contain:\\n', len(p_val),'ECMs\\t',len(n_val),'Non-ECMs')\n",
        "        print('Test Dataset contain:\\n', len(p_test),'ECMs\\t',len(n_test),'Non-ECMs')\n",
        "\n",
        "        model = ECM_LSE_Final_Model(LV,input_size=X_train.shape[1])\n",
        "\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=100)\n",
        "\n",
        "        checkpoint = ModelCheckpoint('models\\\\model-best' + str(Gap) + '_LV' + str(LV) + '.h5',\n",
        "                                      verbose=0, monitor='val_class_output_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "\n",
        "        history = model.fit({'enc_input': X_train},\n",
        "                            {'class_output': y_train, 'decoder_output': X_train},\n",
        "                            validation_data = ({'enc_input': X_val},\n",
        "                            {'class_output': y_val, 'decoder_output': X_val}),\n",
        "                            epochs=1000, batch_size=100, callbacks=[checkpoint, es], verbose=0)\n",
        "\n",
        "        del model  # deletes the existing model\n",
        "        model = load_model('models\\\\model-best' + str(Gap) + '_LV' + str(LV) + '.h5')\n",
        "\n",
        "        y_train_pred, X_train_pred = model.predict(X_train,batch_size=540, verbose=0)\n",
        "        y_train_pred = to_categorical(y_train_pred.argmax(axis=1))\n",
        "        MSE_X_train_pred = (np.square(X_train_pred - X_train)).mean(axis=1)\n",
        "\n",
        "        y_val_pred, X_val_pred = model.predict(X_val,batch_size=60, verbose=0)\n",
        "        y_val_pred = to_categorical(y_val_pred.argmax(axis=1))\n",
        "        MSE_X_val_pred = (np.square(X_val_pred - X_val)).mean(axis=1)\n",
        "\n",
        "        y_test_pred, X_test_pred = model.predict(X_test,batch_size=3887, verbose=0)\n",
        "        y_test_pred = to_categorical(y_test_pred.argmax(axis=1))\n",
        "        MSE_X_test_pred = (np.square(X_test_pred - X_test)).mean(axis=1)\n",
        "\n",
        "        \n",
        "        ## Training Measures\n",
        "        tr_acc = accuracy_score(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "        tr_sen = pmeasure(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))['Sensitivity']\n",
        "        tr_spe = pmeasure(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))['Specificity']\n",
        "        tr_f1 = pmeasure(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))['F1-Score']\n",
        "        tr_mcc = matthews_corrcoef(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "        tr_bacc = balanced_accuracy_score(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "        tr_yi = yoden_index(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "\n",
        "        ## Validation Measures\n",
        "        v_acc = accuracy_score(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "        v_sen = pmeasure(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))['Sensitivity']\n",
        "        v_spe = pmeasure(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))['Specificity']\n",
        "        v_f1 = pmeasure(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))['F1-Score']\n",
        "        v_mcc = matthews_corrcoef(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "        v_bacc = balanced_accuracy_score(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "        v_yi = yoden_index(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "\n",
        "        ## Test Measures\n",
        "        t_acc = accuracy_score(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "        t_sen = pmeasure(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))['Sensitivity']\n",
        "        t_spe = pmeasure(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))['Specificity']\n",
        "        t_f1 = pmeasure(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))['F1-Score']\n",
        "        t_mcc = matthews_corrcoef(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "        t_bacc = balanced_accuracy_score(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "        t_yi = yoden_index(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "\n",
        "\n",
        "        Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi, -10*np.log10(MSE_X_train_pred.mean()),\n",
        "                      v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi, -10*np.log10(MSE_X_val_pred.mean()),\n",
        "                      t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, -10*np.log10(MSE_X_test_pred.mean())])\n",
        "        print('Gap(k)=',Gap,'LV=',LV,'Trial:',loop_ind,'Training-Youden index:',tr_yi, 'Validation-Youden index:',v_yi, 'Test-Youden index:',t_yi)\n",
        "    Statistics = np.asarray(Stats)\n",
        "    filename = 'ECM_LSE_results_GAP' + str(Gap) + '_LV' + str(LV) + 'x.mat'\n",
        "    savemat(filename,{'Statistics':Statistics})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddwzm0NGaFsy"
      },
      "source": [
        "## Show Classification/Reconstruction Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSXF9ZkXXv3Y"
      },
      "source": [
        "## Show Classification/Reconstruction Statistics\n",
        "Show_Statistics('Training Results (MEAN)',Statistics.mean(axis=0)[0:8])\n",
        "Show_Statistics('Validation Results (MEAN)',Statistics.mean(axis=0)[8:16])\n",
        "Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[16:24])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}